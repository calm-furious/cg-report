<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Computer Graphics - Final Project</title>

    <link href="resources/bootstrap.min.css" rel="stylesheet">
    <link href="resources/offcanvas.css" rel="stylesheet">
    <link href="resources/custom2014.css" rel="stylesheet">
    <link href="resources/twentytwenty.css" rel="stylesheet" type="text/css" />
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
  </script>

  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <style>
        img {
            width: 800px;
          }          
    </style>
</head>
<body>
<div class="container headerBar">
		<h2>Final Project - Junchi Chen & Xiyi Chen</h2>
</div>

<div class="container contentWrapper">
<div class="pageContent">

<h1 class="title">Motivation Images</h1>
<img src="images/motiv1.png" alt="1" style="width:400px"/>
<img src="images/motiv2.jpg" alt="2" style="width:534px"/>

We basically want a image that's in the sky with something that normally is not in the sky presenting.
In the first image, the person is sitting on the clouds and holding the airplane, which is clearly out of place.  
In the second image, the wools are fused with the clouds.

<h1 class="title">Junchi Chen's feature:</h1>
<h2 id="euler">5.19 Rendering using the Euler Cluster</h2>
<p><strong>Time spent:</strong> 5hr</p>
<p><strong>Problems occured:</strong></p>
<ol type="1">
<li>Didn’t correctly initialize RenderThread ( nori exit after
block.put(m_block)… ) .  Now fixed with thread join.</li>

<li>When to join thread correctly? Now putting in deconstuctor of render object</li>

</ol>
<h3 id="implementation">Implementation</h3>
<p>Get rid of the gui part. Basicly get rid of anything related to
nanogui, gui.cpp.</p>
<p>In main function, I replaced NoriScreen with RenderThread directly
due to maybe unsuccessful initialize of threads otherwise.</p>

<h3 id="test-on-euler">Test on Euler</h3>
<p>Compiling:</p>
<img src="images/cjc/euler/compile.png" alt="Compiling"/> <br> <br> <br> <br>
<p>Submitting:</p>
<img src="images/cjc/euler/submit.png" alt="Test results"/> <br> <br> <br> <br>
<p>Loading:</p>
<img src="images/cjc/euler/loading_scene.png" alt="Test results"/> <br> <br> <br> <br>
<p>result:</p>
<div class="twentytwenty-container">
  <img src="images/cjc/euler/veach_mis.png" alt="Euler" class="img-responsive">
  <img src="images/cjc/euler/veach_mis_old.png" alt="Laptop" class="img-responsive">
</div>

<h2 id="environment-mapped-emitter">15.3 Environment Mapped Emitter</h2>
<p><strong>Time spent:</strong> 28hr</p>
<p><strong>Ref:</strong> Humphreys, G.R., &amp; Phare, M. (2012). Monte
Carlo Rendering with Natural Illumination.</p>
<p><strong>Problems:</strong></p>
<ol type="1">
<li>how to map image to sphere?
<ol type="1">
<li>coordinate transformation, direction(x,y,z)→ (<span class="math inline">\(\theta\)</span>, <span class="math inline">\(\phi\)</span>) → (u,v) on img</li>
</ol></li>
<li>how to sample it?
<ul>
<li>refer to paper, precompute a piece-wise 2D density map, then sample
accordingly</li>
</ul></li>
<li>some segfaults on cluster
<ul>
<li><code>sbatch --time=0:30:00 --ntasks=6 --wrap=&quot;gdb --batch --eval-command=run --eval-command=bt --nw --args ./nori …&quot;</code></li>
<li>use gdb command as above, and found some negative img uv…</li>
</ul></li>
</ol>
<h3 id="implementation-1">Implementation</h3>
<h5 id="constructor-pre-computation">Constructor: pre-computation</h5>
<p>Here I do the pre-computation of the discrete 2D distribution in the
constructor of the enviorn-map emitter. Also compute the 1D marginal
distribution for simplicity in sampling.</p>
<p>Modularity of precompte1D is not easy to achieve: A row of matrix
cannot be easily cast to Eigen::Array&lt;float, Eigen::Dynamic, 1,
Eigen::RowMajor&gt; as parameter, not sure why.</p>
<h5 id="sampling">Sampling:</h5>
<p>According to the precomputed 2D distribution and 1D marginal
distribution, use 2D uniform square sample to sample the environ-map
image.</p>
<h3 id="building-test-scene-mitsuba">Building Test Scene (mitsuba)</h3>
<p>The camera coordinates are not exactly the same nori, which costs
some more time.</p>
<p>Created similar scene: 2 balls, 1 mirror and 1 dielectric, with Grace
Cathedral inner image as environ-map.</p>
<h3 id="results">Results</h3>
<p>spp=64, I flipped nori's output horizontally due to convention difference.</p>
<div class="twentytwenty-container">
  <img src="images/cjc/envmap/2mirror.png" alt="nori" class="img-responsive">
  <img src="images/cjc/envmap/mit2mirror.png" alt="mitsuba" class="img-responsive">
</div>

<h2 id="disney-bsdf">15.5 Disney BSDF</h2>
<p>The params of choice: metallic, anisotropic, clearcoat, specular,
roughness.</p>
<p><strong>Time spent:</strong> 30hr <strong>
<br>  
Ref:</strong> 1. Brent
Burley. Physically-based shading at Disney. In SIGGRAPH Course, 2012. <br>  

2.
Eric Heitz. Sampling the GGX distribution of visible normals. J. Comput.
Graph. Tech, 7(4), 2018. <br>  
3. UCSD CSE 272 Assignment 1: Disney Principled
BSDF</p>
<p>Problems: 1. anisotropic has some non-continuous boarder for meshes <br>  
- Use the same tangent calculation for the analytical sphere, and I solved
the problem.</p>
<h3 id="implementation-2">Implementation</h3>
<p>The total bsdf function of the 5 params can be separated into a few
lobes:</p>
<ol type="1">
<li>Diffuse lobe
<ul>
<li>sample with cosine hemisphere</li>
</ul></li>
<li>metallic lobe
<ul>
<li>importance sample the function <span class="math inline">\(\frac{D_m
G(\omega_{\mathrm{in}})}{4|n\cdot\omega_{\mathrm{in}}|}\)</span></li>
</ul></li>
<li>clearcoat
<ul>
<li>importance sample the function <span class="math inline">\(\frac{D_c|n\cdot
h|}{4|h\cdot\omega_{out}|}\)</span></li>
</ul></li>
</ol>
<h3 id="validation">Validation:</h3>
<p> Roughness </p>
<img src="images/cjc/disney/5rough.png" alt="clearcoat=0"/>  <br> <br> <br> <br>
<p> metallic </p>
<img src="images/cjc/disney/5metallic.png" alt="clearcoat=0"/>  <br> <br> <br> <br>
<p> anisotropic </p>
<img src="images/cjc/disney/5ani.png" alt="clearcoat=0"/>  <br> <br> <br> <br>
<p> specular </p>
<img src="images/cjc/disney/5specular.png" alt="clearcoat=0"/> <br> <br> <br> <br>


<p> clearcoat (clearcoatGloss=1.f), this lobe hardly make a difference so I constructor another comparison. </p>
<div class="twentytwenty-container">
  <img src="images/cjc/disney/clear0.png" alt="clearcoat=0" class="img-responsive">
  <img src="images/cjc/disney/clear3.png" alt="clearcoat=0.3" class="img-responsive">
  <img src="images/cjc/disney/clear6.png" alt="clearcoat=0.6" class="img-responsive">
  <img src="images/cjc/disney/clear1.png" alt="clearcoat=1" class="img-responsive">
</div>

<h2 id="advanced-camera-model">15.8+5.1 Advanced camera model</h2>
<p>Time spent: 15hr</p>
<p>Ref: PBRT</p>
<p>Implemented 4 simulations of real camera defects, namely: Depth of
Field, Distortion, Color Aberration and non-spherical aperture.</p>
<h3 id="implementation-and-results">Implementation and Results</h3>
<ol type="1">
<li>Depth of Field is implemented according to PBRT chapter 6.2,
basically replace the original camera ray with a ray that is refracted
by the lens. Here we need 2 more variables, namely: focal distance and
radius of aperture.</li>
<p> Comparison: </p>
<div class="twentytwenty-container">
  <img src="images/cjc/camera/dof1.png" alt="Depth of Field1" class="img-responsive">
  <img src="images/cjc/camera/dof2.png" alt="Depth of Field2" class="img-responsive">
  <img src="images/cjc/camera/wodof.png" alt="Original" class="img-responsive">
</div>

<li>non-spherical aperture: the only modification is change the lens
sampling methods. Previously to implement depth of field, I sampled a
disk concentriclly using method discussed in PBRT chapter 13.6.2. Now we
are not sampling a disk anymore. I, according to my personal interest,
want to introduce 2 shapes. The first is square, which I only need to
provide a uniform square sampling function, which we already have. The
second is a heart shape, this is done by rejection sampling using a
heart shaped equation.</li>
<p> Comparison: </p>
<div class="twentytwenty-container">
  <img src="images/cjc/camera/heart.png" alt="heart" class="img-responsive">
  <img src="images/cjc/camera/square.png" alt="square" class="img-responsive">
  <img src="images/cjc/camera/disk.png" alt="disk" class="img-responsive">
  <img src="images/cjc/camera/none.png" alt="normal" class="img-responsive">
</div>

<li>Distortion and 4. Color Aberration: is implemented by heuristics,
that is: first calculate the distance of the current pixel point to the
center of the image. Then according to the squared distance, displace
this pixel value. This is done by changing the sampled point on the near
plane.
<br>
For color aberrration, we need different distortion for different channel. 
So, some modification in camera.h is needed.
</li>
<p> Comparison: </p>
<div class="twentytwenty-container">
  <img src="images/cjc/camera/dist.png" alt="distortion" class="img-responsive">
  <img src="images/cjc/camera/ca.png" alt="color aberration" class="img-responsive">
  <img src="images/cjc/camera/none2.png" alt="normal" class="img-responsive">
</div>


</ol>
<h2 id="building-my-own-mesh">5.20 Building my own mesh</h2>
<p>Time spent: 5hr</p>
<p>I made a hot air balloon. This is during modeling:</p>
<img src="images/cjc/mesh/modeling.png" alt="modeling"/> <br> <br> <br> <br>
<p>This is the result:</p>
<img src="images/cjc/mesh/result1.png" alt="result"/> <br> <br> <br> <br>
<img src="images/cjc/mesh/result.png" alt="result"/> <br> <br> <br> <br>


<h1 class="title">Xiyi Chen's feature:</h1>

<h2 id="euler">5.2 Motion blur for cameras</h2>
<p><strong>Time spent:</strong> ~1hr</p>

<h3 id="implementation-2">Implementation</h3>
<p>Sample a time t, given a camera to world transformation T1 and camera moving direction transformation T2, build a new transformation T' = T1*(1-t) + T2*t if using camera motion blur. </p>
<p>Files changed: camera.h, perspective.cpp, simcamera.cpp.</p>

<div class="twentytwenty-container">
  <img src="images/cxy/motionblur/cbox_path_mis.png" alt="without_motion_blur" class="img-responsive">
  <img src="images/cxy/motionblur/cbox_path_mis_motion_blur.png" alt="with_motion_blur" class="img-responsive">
</div>

<h2 id="euler">5.3 Images as Textures</h2>
<p><strong>Time spent:</strong> ~2hrs</p>

<h3 id="implementation-2">Implementation</h3>
<p>I use lodepng library (https://github.com/lvandeve/lodepng) to load a png texture file as RGBA, and calculate the scaled u and v coordinates (as in checkerboard), and then convert them back to width / height in the image space to fetch the corresponding pixels. </p>
<p>Files changed/added: imageastexture.cpp, lodepng.cpp, lodepng.h.</p>

<div class="twentytwenty-container">
  <img src="images/cxy/imageastexture/mesh-texture.png" alt="mine" class="img-responsive">
  <img src="images/cxy/imageastexture/texture_mitsuba.png" alt="mitsuba reference" class="img-responsive">
</div>

<h2 id="euler">15.8 Moderate Denoising 1: NL-means using Pixel Variance Estimates</h2>
<p><strong>Time spent:</strong> ~10hrs</p>

<h3 id="implementation-2">Implementation</h3>
<p>I calculate the variance map during rendering process following the pseudocode for variance estimation in the slides. After that, I use a simple python script to apply nl-means denoising following the pseudocode of "NL-Means – Fast Implementation" in the slides. </p>
<p>Files changed/added: main.cpp, render.cpp, nl_means_denoiser.py, nl_means_denoising_opencv.py.</p>

<div class="twentytwenty-container">
  <img src="images/cxy/denoise/cbox_path_mis.png" alt="render results" class="img-responsive">
  <img src="images/cxy/denoise/cbox_path_mis_var.png" alt="variance" class="img-responsive">
</div>

<div class="twentytwenty-container">
  <img src="images/cxy/denoise/cbox_path_mis_denoised_opencv.png" alt="mine, denoised" class="img-responsive">
  <img src="images/cxy/denoise/cbox_path_mis_denoised.png" alt="opencv reference" class="img-responsive">
</div>

<h2 id="euler">30.1 Heterogeneous Volumetric Participating Media</h2>
<p><strong>Time spent:</strong> ~40hrs</p>

<h3 id="implementation-2">Implementation</h3>
<p>I implemented heterogeneous participating media in the entire scenes and bound it as a box using input origin/dimension parameters. The distance sampling and transmittance functions refer to mitsuba implementation. To medium interaction, we move t along the ray by -log(1 - random) * density / sigma_t, and terminate with a failure status if it goes beyond tmax or moves out of the bounding box. Otherwise, we record the point at t and return the albedo times density if albedo / density is larger than a random number. 
The volumetric path tracing integrator extends on path_mis from the last homework. For volume interaction, the pdf of the material sampling and wo are sampled from the phase function. For surface interaction, pdf_mats comes from bsdf and everything basically follows the path mis integrator, except that there's a transmittance term multiplied to every light contribution. Russian roulette is performed in both volume/surface interaction. In addition, after setting the new ray, I check if there's an intersection and update w_mats. For phase function, I only had time to do the isotropic one. </p>
<p>Files changed/added: medium.h, PhaseFunction.h, scene.h/.cpp, isotropic.cpp, heterogeneous.cpp, volpath_mis.cpp</p>

<div class="twentytwenty-container">
  <img src="images/cxy/medium/cbox_path_mis_hetero.png" alt="mine" class="img-responsive">
  <img src="images/cxy/medium/scene_hetero_mitsuba.png" alt="mitsuba" class="img-responsive">
</div>

<h1 class="title">Final Image</h1>

<p>The image presenting following, is the final result, where camera motion, camera color aberration, disney brdf, environment-mapped-emitter and self model is applied. </p>
<p>A polybahn in the sky is appearently out of place. </p>
<img src="images/final.png" alt="final"/>

</div>
</div>



<!-- Bootstrap core JavaScript -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="resources/bootstrap.min.js"></script>
<script src="resources/jquery.event.move.js"></script>
<script src="resources/jquery.twentytwenty.js"></script>


<script>
$(window).load(function(){$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5});});
</script>

</body>
</html>